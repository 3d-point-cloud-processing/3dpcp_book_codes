{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from torch_geometric.datasets import S3DIS\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "current_path = Path.cwd()\n",
    "dataset_dir = current_path / \"S3DIS\"\n",
    "\n",
    "train_dataset = S3DIS(dataset_dir, test_area=6, train=True, transform=None, pre_transform=None, pre_filter=None)\n",
    "test_dataset = S3DIS(dataset_dir, test_area=6, train=False, transform=None, pre_transform=None, pre_filter=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train_dataset len:\", len(train_dataset))\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import global_max_pool\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import DataLoader as DataLoader\n",
    "\n",
    "class InputTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InputTNet, self).__init__()\n",
    "        self.input_mlp = nn.Sequential(\n",
    "            nn.Linear(3, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
    "            nn.Linear(64, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Linear(128, 1024), nn.BatchNorm1d(1024), nn.ReLU(),\n",
    "        )\n",
    "        self.output_mlp = nn.Sequential(\n",
    "            nn.Linear(1024, 512), nn.BatchNorm1d(512), nn.ReLU(),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(),\n",
    "            nn.Linear(256, 9)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, batch):\n",
    "        x = self.input_mlp(x)\n",
    "        x = global_max_pool(x, batch)\n",
    "        x = self.output_mlp(x)\n",
    "        x = x.view(-1, 3, 3)\n",
    "        id_matrix = torch.eye(3).to(x.device).view(1, 3, 3).repeat(x.shape[0], 1, 1)\n",
    "        x = id_matrix + x\n",
    "        return x\n",
    "\n",
    "class FeatureTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureTNet, self).__init__()\n",
    "        self.input_mlp = nn.Sequential(\n",
    "            nn.Linear(64, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
    "            nn.Linear(64, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Linear(128, 1024), nn.BatchNorm1d(1024), nn.ReLU(),\n",
    "        )\n",
    "        self.output_mlp = nn.Sequential(\n",
    "            nn.Linear(1024, 512), nn.BatchNorm1d(512), nn.ReLU(),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(),\n",
    "            nn.Linear(256, 64*64)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, batch):\n",
    "        x = self.input_mlp(x)\n",
    "        x = global_max_pool(x, batch)\n",
    "        x = self.output_mlp(x)\n",
    "        x = x.view(-1, 64, 64)\n",
    "        id_matrix = torch.eye(64).to(x.device).view(1, 64, 64).repeat(x.shape[0], 1, 1)\n",
    "        x = id_matrix + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNetSegmentation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PointNetSegmentation, self).__init__()\n",
    "        self.input_tnet = InputTNet()\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Linear(3, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
    "            nn.Linear(64, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
    "        )\n",
    "        self.feature_tnet = FeatureTNet()\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(64, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
    "            nn.Linear(64, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Linear(128, 1024), nn.BatchNorm1d(1024), nn.ReLU(),\n",
    "        )\n",
    "        self.mlp3 = nn.Sequential(\n",
    "            nn.Linear(1088, 512), nn.BatchNorm1d(512), nn.ReLU(),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(),\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Linear(128, 13)\n",
    "        )\n",
    "        \n",
    "    def forward(self, batch_data):\n",
    "        x = batch_data.pos\n",
    "        \n",
    "        input_transform = self.input_tnet(x, batch_data.batch)\n",
    "        transform = input_transform[batch_data.batch, :, :]\n",
    "        x = torch.bmm(transform, x.view(-1, 3, 1)).view(-1, 3)\n",
    "        \n",
    "        x = self.mlp1(x)\n",
    "        \n",
    "        feature_transform = self.feature_tnet(x, batch_data.batch)\n",
    "        transform = feature_transform[batch_data.batch, :, :]\n",
    "        x = torch.bmm(transform, x.view(-1, 64, 1)).view(-1, 64)\n",
    "        pointwise_feature = x\n",
    "        \n",
    "        x = self.mlp2(x)        \n",
    "        x = global_max_pool(x, batch_data.batch)\n",
    "        global_feature = x[batch_data.batch, :]\n",
    "        \n",
    "        x = torch.cat([pointwise_feature, global_feature], axis=1)\n",
    "        x = self.mlp3(x)\n",
    "        \n",
    "        return x, input_transform, feature_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "num_epoch = 400\n",
    "batch_size = 16\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = PointNetSegmentation()\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(lr=1e-4, params=model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=num_epoch // 4, gamma=0.5)\n",
    "\n",
    "log_dir = current_path / \"log_S3DIS_segmentation\"\n",
    "log_dir.mkdir(exist_ok=True)\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "criteria = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    model = model.train()\n",
    "    \n",
    "    losses = []\n",
    "    for batch_data in tqdm(train_dataloader, total=len(train_dataloader)):\n",
    "        batch_data = batch_data.to(device)\n",
    "        this_batch_size = batch_data.batch.detach().max() + 1\n",
    "        \n",
    "        pred_y, _, feature_transform = model(batch_data)\n",
    "        true_y = batch_data.y.detach()\n",
    "\n",
    "        class_loss = criteria(pred_y, true_y)\n",
    "        accuracy = float((pred_y.argmax(dim=1) == true_y).sum()) / float(this_batch_size)\n",
    "\n",
    "        id_matrix = torch.eye(feature_transform.shape[1]).to(feature_transform.device).view(1, 64, 64).repeat(feature_transform.shape[0], 1, 1)\n",
    "        transform_norm = torch.norm(torch.bmm(feature_transform, feature_transform.transpose(1, 2)) - id_matrix, dim=(1, 2))\n",
    "        reg_loss = transform_norm.mean()\n",
    "\n",
    "        loss = class_loss + reg_loss * 0.001\n",
    "        \n",
    "        losses.append({\n",
    "            \"loss\": loss.item(),\n",
    "            \"class_loss\": class_loss.item(),\n",
    "            \"reg_loss\": reg_loss.item(),\n",
    "            \"accuracy\": accuracy,\n",
    "            \"seen\": float(this_batch_size)})\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    if (epoch % 10 == 0):\n",
    "        model_path = log_dir / f\"model_{epoch:06}.pth\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    loss = 0\n",
    "    class_loss = 0\n",
    "    reg_loss = 0\n",
    "    accuracy = 0\n",
    "    seen = 0\n",
    "    for d in losses:\n",
    "        seen = seen + d[\"seen\"]\n",
    "        loss = loss + d[\"loss\"] * d[\"seen\"]\n",
    "        class_loss = class_loss + d[\"class_loss\"] * d[\"seen\"]\n",
    "        reg_loss = reg_loss + d[\"reg_loss\"] * d[\"seen\"]\n",
    "        accuracy = accuracy + d[\"accuracy\"] * d[\"seen\"]\n",
    "    loss = loss / seen\n",
    "    class_loss = class_loss / seen\n",
    "    reg_loss = reg_loss / seen\n",
    "    accuracy = accuracy / seen\n",
    "    writer.add_scalar(\"train_epoch/loss\", loss, epoch)\n",
    "    writer.add_scalar(\"train_epoch/class_loss\", class_loss, epoch)\n",
    "    writer.add_scalar(\"train_epoch/reg_loss\", reg_loss, epoch)\n",
    "    writer.add_scalar(\"train_epoch/accuracy\", accuracy, epoch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model = model.eval()\n",
    "\n",
    "        losses = []\n",
    "        for batch_data in tqdm(test_dataloader, total=len(test_dataloader)):\n",
    "            batch_data = batch_data.to(device)\n",
    "            this_batch_size = batch_data.batch.detach().max() + 1\n",
    "\n",
    "            pred_y, _, feature_transform = model(batch_data)\n",
    "            true_y = batch_data.y.detach()\n",
    "\n",
    "            class_loss = criteria(pred_y, true_y)\n",
    "            accuracy =float((pred_y.argmax(dim=1) == true_y).sum()) / float(this_batch_size)\n",
    "\n",
    "            id_matrix = torch.eye(feature_transform.shape[1]).to(feature_transform.device).view(1, 64, 64).repeat(feature_transform.shape[0], 1, 1)\n",
    "            transform_norm = torch.norm(torch.bmm(feature_transform, feature_transform.transpose(1, 2)) - id_matrix, dim=(1, 2))\n",
    "            reg_loss = transform_norm.mean()\n",
    "\n",
    "            loss = class_loss + reg_loss * 0.001 * 0.001\n",
    "\n",
    "            losses.append({\n",
    "                \"loss\": loss.item(),\n",
    "                \"class_loss\": class_loss.item(),\n",
    "                \"reg_loss\": reg_loss.item(),\n",
    "                \"accuracy\": accuracy,\n",
    "                \"seen\": float(this_batch_size)})\n",
    "            \n",
    "        loss = 0\n",
    "        class_loss = 0\n",
    "        reg_loss = 0\n",
    "        accuracy = 0\n",
    "        seen = 0\n",
    "        for d in losses:\n",
    "            seen = seen + d[\"seen\"]\n",
    "            loss = loss + d[\"loss\"] * d[\"seen\"]\n",
    "            class_loss = class_loss + d[\"class_loss\"] * d[\"seen\"]\n",
    "            reg_loss = reg_loss + d[\"reg_loss\"] * d[\"seen\"]\n",
    "            accuracy = accuracy + d[\"accuracy\"] * d[\"seen\"]\n",
    "        loss = loss / seen\n",
    "        class_loss = class_loss / seen\n",
    "        reg_loss = reg_loss / seen\n",
    "        accuracy = accuracy / seen\n",
    "        writer.add_scalar(\"test_epoch/loss\", loss, epoch)\n",
    "        writer.add_scalar(\"test_epoch/class_loss\", class_loss, epoch)\n",
    "        writer.add_scalar(\"test_epoch/reg_loss\", reg_loss, epoch)\n",
    "        writer.add_scalar(\"test_epoch/accuracy\", accuracy, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
