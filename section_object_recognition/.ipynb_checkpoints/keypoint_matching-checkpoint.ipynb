{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# キーポイントマッチングによる位置姿勢認識\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import copy\n",
    "o3d.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特定物体の姿勢推定について説明します．\n",
    "画像中に映った物体の三次元的な位置姿勢（並進と回転の合計6パラメータ）の推定は，\n",
    "古くから研究されてきた，コンピュータビジョン分野における基本的な課題の一つです．\n",
    "三次元位置姿勢（以降，単に姿勢と記述します．）を認識することができれば，\n",
    "ロボットアームによって，その物体を掴んだり操作することができます．\n",
    "また，カメラの動きに合わせてリアルなCGを画像に合成する，いわゆるARアプリケーションや，\n",
    "異なる視点で撮影された点群同士を張り合わせて大規模な環境地図を生成することも可能になります．\n",
    "\n",
    "本節では，広く利用されているキーポイントマッチングを題材として，\n",
    "異なる視点で撮影された２つの点群を入力として，これらを貼り合わせるためのアルゴリズムを紹介します．\n",
    "\n",
    "キーポイントマッチングの目的は，\n",
    "入力された2つの点群（source,targetとします）を貼り合わせる変換行列を求めることです．\n",
    "変換行列は，$4\\times4$の同次変換行列${\\bf T} = [{\\bf R},{\\bf t};{\\bf 0}, 1]$で表現されることが一般的です．\n",
    "ここで，${\\bf R}$ は$3 \\times 3$の回転行列，${\\bf t}$は$3 \\times 1$の平行移動ベクトルです．\n",
    "第3章で紹介したICPアルゴリズムの目的もこれと同様ですが，ICPアルゴリズムは2つの点群の初期位置がかなり近いことを前提としていますが，キーポイントマッチングではそれを仮定していません．\n",
    "より一般的なシーンで利用できる利点があります．\n",
    "推定する位置姿勢の精度が求められる場合は，キーポイントマッチングで得られた位置姿勢を初期値として，ICPアルゴリズムを適用することが多いです．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## キーポイントマッチングによる6DoF姿勢推定\n",
    "位置姿勢は6自由度あるため，その探索空間は膨大です．\n",
    "sourceの位置姿勢を逐次変更しながらtargetと照合すると計算コストが非常に高くなるため，現実的ではありません．\n",
    "\n",
    "そこで，sourceとtargetのよく似た部分的な領域を見つけ出して，その情報をもとに姿勢を計算します．\n",
    "部分的な領域の類似性を計算するので，効率的な姿勢計算が可能になります．\n",
    "これがキーポイントマッチングによる姿勢推定です．\n",
    "この手法は下記の4つのステップで処理がおこなわれます．\n",
    "\n",
    " 1. キーポイント検出\n",
    " 2. 特徴量記述\n",
    " 3. 対応点探索\n",
    " 4. 姿勢計算\n",
    "\n",
    "各ステップをOpen3Dによって実装しましょう．\n",
    "\n",
    "### キーポイント検出\n",
    "まずは，sourceとtargetの両データに対してキーポイントを検出します．\n",
    "特徴点に関しては，ISSのように物体表面の物理的な凹凸部分もとにキーポイントを検出する手法や，\n",
    "単に等間隔にサンプリングをおなった点をキーポイントとする手法が存在します．\n",
    "ISSに関しては第3章に説明がありますので，今回は等間隔サンプリングによってキーポイントを選択しましょう．\n",
    "ここでは，Voxel Grid Filterを利用します．\n",
    "\n",
    "## 特徴量記述\n",
    "特徴量記述はキーポイントに対してそのキーポイントらしさを表現する情報（アイデンティティ）を付与する処理です．\n",
    "キーポイント周りの形状をもとに計算した多次元ベクトルを特徴量とすることが一般的です．\n",
    "特徴量に関する説明は第3章にあります．\n",
    "今回は，FPFH特徴量を利用します．\n",
    "\n",
    "キーポイント検出と特徴量記述は，sourceとtargetの両方の点群に適用する処理なので，ここでは関数として実装します．FPFH特徴量の計算には法線が必要ですので，計算することにします．\n",
    "まずはマッチング対象の点群を読み込んで表示しましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データ読み込み\n",
    "path = \"../3rdparty/Open3D/examples/test_data/ICP/\"\n",
    "source = o3d.io.read_point_cloud(path+\"cloud_bin_0.pcd\")\n",
    "target = o3d.io.read_point_cloud(path+\"cloud_bin_1.pcd\")\n",
    "\n",
    "source.paint_uniform_color([0.5,0.5,1])\n",
    "target.paint_uniform_color([1,0.5,0.5])\n",
    "initial_trans = np.identity(4)\n",
    "initial_trans[0,3] = -3.0\n",
    "\n",
    "def draw_registration_result( source, target, transformation ):\n",
    "    pcds = list()\n",
    "    for s in source: \n",
    "        temp = copy.deepcopy(s)\n",
    "        pcds.append( temp.transform(transformation) )\n",
    "    pcds += target\n",
    "    o3d.visualization.draw_geometries(pcds, zoom=0.3199,\n",
    "                                      front = [0.024, -0.225, -0.973],\n",
    "                                      lookat = [0.488, 1.722, 1.556],\n",
    "                                      up = [0.047, -0.972, 0.226])\n",
    "\n",
    "draw_registration_result( [source], [target], initial_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```draw_registration_result```は，点群を画面表示するための関数です．\n",
    "第１引数の点群(リストとして渡します．)を第３引数の姿勢変換のための行列によって変換し，第２引数の点群(こちらもリストとして渡します．)と同時に表示します．\n",
    "ここでは，見やすさのために```source```をx軸方向に-3.0だけ平行移動しています．\n",
    "\n",
    "\n",
    "次に，これら２つの点群から，キーポイントを検出し，特徴量を計算しましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keypoint_and_feature_extraction( pcd, voxel_size ):\n",
    "\n",
    "    keypoints = pcd.voxel_down_sample(voxel_size)\n",
    "\n",
    "    viewpoint = np.array([0.,0.,0.], dtype='float64')\n",
    "    radius_normal = 2.0*voxel_size\n",
    "    keypoints.estimate_normals(\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius_normal, max_nn=30))\n",
    "    keypoints.orient_normals_towards_camera_location( viewpoint )\n",
    "\n",
    "    radius_feature = 5.0*voxel_size\n",
    "    feature = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "        keypoints,\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius_feature, max_nn=100))\n",
    "    return keypoints, feature\n",
    "\n",
    "voxel_size = 0.1\n",
    "s_kp, s_feature = keypoint_and_feature_extraction( source, voxel_size )\n",
    "t_kp, t_feature = keypoint_and_feature_extraction( target, voxel_size )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "関数```keypoint_and_feature_extraction()```は引数として```pcd```と```voxel_size```をとります．\n",
    "```pcd```はこの処理を適用する点群であり，```voxel_size```はVoxel Grid Filterのボクセルサイズ，すなわち，検出するキーポイントの間隔です．\n",
    "3行目でVoxel Grid Filterによって間引いた点群をキーポイントとします．\n",
    "5--9行目は点群に対して法線を計算しています．まず，法線を計算するための範囲として，間引いたデータの倍の半径を指定し，その範囲内で最大30点を使って計算する設定にしています．\n",
    "9行目は適当な視点（カメラ視点の場合が多いですが，今回は原点としています．）を仮定し，その方向に法線が向くように法線方向に反転処理を適用しています．\n",
    "\n",
    "11--14行目で特徴量を計算しています．\n",
    "特徴量の計算範囲は，その後の処理の性能に大きな影響を与えるため，重要です．\n",
    "特徴量を計算するために十分な点数や範囲を含むことを意識して設定しましょう．\n",
    "ここでは，キーポイント間隔5倍の距離を半径を限度とした領域に含まれる点のうち，近いものから順に最大100点を選択することにしました．\n",
    "12行目で使っている関数は，キーポイントと計算範囲のみを入力としてFPFH特徴量を計算しています．本来，FPFH特徴量を計算するには，キーポイント，特徴量を計算するために使う点群（キーポイントを選ぶ前の元の点群），計算範囲の3つの情報が必要です．この関数では，キーポイントから特徴量を計算していることになるので，厳密にはFPFH特徴量の計算方法とは異なりますが，キーポイントをVoxel Grid Filterで算出している（＝もとの点群とほぼ同じ空間分布の点群を使っている）ため，問題が少ないと思われます．\n",
    "ちなみに，Point Cloud Library(PCL)の3D特徴量計算の関数は，キーポイントと特徴量を計算するために使う点群を分けて入力するように実装されています．\n",
    "\n",
    "キーポイントを緑に着色して，もとの点群と一緒に表示します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_kp.paint_uniform_color([0,1,0])\n",
    "t_kp.paint_uniform_color([0,1,0])\n",
    "draw_registration_result([source,s_kp], [target,t_kp], initial_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図の通り，元の点群に対してまんべんなくキーポイントを選択することができました．\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 対応点探索\n",
    "このステップの目的は物体モデルと入力シーン間で物理的に同一の地点を指す座標同士の対応を得ることです．\n",
    "このために両特徴量の距離を計算します．距離が最も小さいキーポイントのペアが対応点になりますが，\n",
    "誤った対応点を含んでしまう場合があります．\n",
    "\n",
    "この解決法には，いくつかの方法が存在します．\n",
    "まずは，最も単純な方法として，特徴量間のノルムにしきい値を設定する方法があります．\n",
    "しきい値以下のノルムが得られた場合に対応点とみなします．\n",
    "\n",
    "しかしながら，点群には，センサノイズや，オクルージョンによる部分的な欠損が存在しうるので，\n",
    "しきい値設定は簡単でない場合があります．\n",
    "他の方法としては，双方向チェックがあります．sourceからtargetへマッチングした場合のベストマッチと\n",
    "targetからsourceへマッチングした場合のベストマッチが同一であった場合に対応点とみなす方法です．\n",
    "ノルムが比較的大きかったとしても，双方向のベストマッチが一致すれば，それらのキーポイントは物理的同一地点を\n",
    "指している可能性が高いでしょう．\n",
    "\n",
    "さらに，Ratio testと呼ばれる方法があります．\n",
    "この方法では，最近傍のノルムが他と比べて際立って小さいかどうかを調べます．\n",
    "具体的には，最近傍のノルム(Aとします)と第2位のノルム（Bとします）間の比(A/B)を計算し，A/Bがしきい値以下の\n",
    "場合に対応点とする方法です．\n",
    "ノルムが小さかったとしても，他にも類似した特徴量を持つキーポイントがあった場合は誤対応かもしれません．\n",
    "Ratio testではそういった対応点候補を棄却することができます．\n",
    "今回は，Ratio testを試してみましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_s_feature = s_feature.data.T\n",
    "np_t_feature = t_feature.data.T\n",
    "\n",
    "corrs = o3d.utility.Vector2iVector()\n",
    "threshold = 0.9\n",
    "for i,feat in enumerate(np_s_feature):\n",
    "    # source側の特定の特徴量とtarget側の全特徴量間のノルムを計算\n",
    "    distance = np.linalg.norm( np_t_feature - feat, axis=1 )\n",
    "    nearest_idx = np.argmin(distance)\n",
    "    dist_order = np.argsort(distance)\n",
    "    ratio = distance[dist_order[0]] / distance[dist_order[1]]\n",
    "    if ratio < threshold:\n",
    "        corr = np.array( [[i],[nearest_idx]], np.int32 )\n",
    "        corrs.append( corr )\n",
    "\n",
    "print('対応点セットの数：', (len(corrs)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1,2行目では，特徴量ベクトルを取り出して(n,33)行列を作成しています．\n",
    "4行目のcorrsは対応点のセットを保存するための変数です．source，target双方のキーポイントのインデクスを保持します．\n",
    "5行目のfor文内では，sourceの特定のキーポイントの特徴量と，targetの特徴量のL2ノルムを計算しています．\n",
    "ノルムが小さいものから1位と2位の比を計算して，threshold以下であれば，正しい対応点セットとみなして，source, targetのインデクスを保存します．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 姿勢計算について\n",
    "対応点探索によって，複数個のsourceとtargetの同一地点の点のペア（対応点）が得られます．\n",
    "このステップでは，対応点を使ってsourceをtargetに位置合わせする変換を推定します．\n",
    "ここで注意すべきことは，対応点セットには誤りを含む可能性があるということです．\n",
    "対応点探索のステップで，Ratio testを使ったとしても，誤対応点を完全に排除することは困難です．\n",
    "そこで，ロバスト推定法として有名なRANdom SAmple Consensus (RANSAC)を利用することにします．\n",
    "\n",
    "## RANSAC\n",
    "RANSACは，点群処理以外にも広く使われているロバスト推定法の一種です．\n",
    "外れ値が含まれた観測値から，その影響を抑えつつ，モデルパラメータを推定します．\n",
    "ここでは，2次元の点列から直線のパラメータ（傾きと切片）を算出する例をもとに，\n",
    "RANSACの考え方について簡単に説明します．\n",
    "図(a)は初期状態の点列です．ここでは，$y = 0.5x + 2.0$からサンプリングした50点に\n",
    "ガウシアンノイズを付加し，さらにランダムな外れ値を40点追加した状態です．\n",
    "90点全ての点を用いて最小二乗法を用いて求めた傾きと切片をもとに直線を描画した例が図(b)です．\n",
    "推定したパラメータが外れ値の影響を受けていることがわかります．\n",
    "RANSACでは，以下の２つの処理を繰り返すことによって，観測データにより良くフィットしたパラメータを\n",
    "推定します．\n",
    "\n",
    "1. サンプリング\n",
    "   - ランダムに数個の計測点を選択し，モデルパラメータ（傾きと切片）を推定します．\n",
    "2. 評価\n",
    "   - 得られたモデルパラメータの良さを評価します．\n",
    "   例えば，得られた直線から一定の距離（マージン）内にあるデータ点の個数（インライア）を数え，その数をパラメータの良さとします．\n",
    "\n",
    "図(c)(d)はサンプリングの例です．(c)はサンプリングした点（赤色）が外れ値を含むので，黒線で描画したマージン内のインライア点数が少ないです．\n",
    "(d)は比較的良い点をサンプリングした時の結果です．マージン内に多くの点が含まれていることがわかります．\n",
    "\n",
    "RANSACではサンプリング処理のときに，外れ値を除いたデータのみを引き当てることを期待しています．\n",
    "もし，そのようなサンプリングができれば，理想的なモデルパラメータを算出することができるからです．\n",
    "それでは，RANSACを使った姿勢推定をおこないましょう．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANSACによる姿勢計算\n",
    "誤りを含んでいる対応点セットから，正しい対応点のみを取り出して姿勢計算をおこないます．\n",
    "まずは，対応点探索で得られたすべての対応点を確認してみましょう．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lineset_from_correspondences( corrs_set, pcd1, pcd2, \n",
    "                                         transformation=np.identity(4) ):\n",
    "    pcd1_temp = copy.deepcopy(pcd1)\n",
    "    pcd1_temp.transform(transformation) \n",
    "    corrs = np.asarray(corrs_set)\n",
    "    np_points1 = np.array(pcd1_temp.points)\n",
    "    np_points2 = np.array(pcd2.points)\n",
    "    points = list()\n",
    "    lines = list()\n",
    "\n",
    "    for i in range(corrs.shape[0]):\n",
    "        points.append( np_points1[corrs[i,0]] )\n",
    "        points.append( np_points2[corrs[i,1]] )\n",
    "        lines.append([2*i, (2*i)+1])\n",
    "\n",
    "    colors = [np.random.rand(3) for i in range(len(lines))]\n",
    "    line_set = o3d.geometry.LineSet(\n",
    "        points=o3d.utility.Vector3dVector(points),\n",
    "        lines=o3d.utility.Vector2iVector(lines),\n",
    "    )\n",
    "    line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "    return line_set\n",
    "\n",
    "line_set = create_lineset_from_correspondences( corrs, s_kp, t_kp, initial_trans )\n",
    "draw_registration_result([source,s_kp], [target,t_kp, line_set], initial_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "対応点をランダムに設定した色の直線で描画しています．\n",
    "関数create_linset_from_correspondencesでは，visualizerで表示する直線群を作成しています．\n",
    "\n",
    "この２つの点群は，ほぼ平行移動した状態で表示されているので，対応点の直線は平行に引かれるのが理想的な状態です．\n",
    "Ratio Testで誤対応の可能性の高いペアは棄却したのですが，斜めに引かれている対応点の直線がいくつか存在することがわかります．これらが外れ値として振る舞います．\n",
    "すべての対応点から姿勢を計算すると，先に説明した直線のパラメータ推定がうまく行かなかったことと同様に，姿勢も誤差を含むということが容易に想像されます．\n",
    "\n",
    "では，RANSACを使う前に，実際にすべての点を使って姿勢計算を行ってみましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_ptp = o3d.pipelines.registration.TransformationEstimationPointToPoint(False)\n",
    "trans_all = trans_ptp.compute_transformation( s_kp, t_kp, corrs )\n",
    "draw_registration_result([source], [target], trans_all )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "クラスo3d.pipelines.registration.TransformationEstimationPointToPointは，2つの対応の取れた点群の二条誤差を最小化する変換行列を算出します．\n",
    "スケーリングを含めた変換を推定可能ですが，今回は位置と姿勢のみを扱いますのでスケーリングは不要です．クラスの引数に与えたFalseはスケーリングを1として計算するためのフラグです．\n",
    "2,3行目で，関数compute_transoformationによって変換行列を算出し，画面に表示しています．\n",
    "外れ値を含む対応点を使って姿勢を計算したので，やはりずれが目立ちます．\n",
    "\n",
    "次に，RANSACを使って，姿勢を計算してみましょう．\n",
    "先に説明した一般的な直線のパラメータを算出する際のRANSACと対比させて説明します．\n",
    "\n",
    "**サンプリング**\n",
    "\n",
    "すべての対応点から，あらかじめ決めておいた個数の対応点を選択し，変換行列を計算します．\n",
    "これには，TransformationEstimationPointToPointを利用します．\n",
    "\n",
    "**評価**\n",
    "\n",
    "得られた変換行列の妥当性を評価します．\n",
    "source側の点群を変換行列によって姿勢変換し，target側の点群との距離を計算します．\n",
    "この値があらかじめ決めておいたマージンより小さい場合はその対応点をインライアとして判定します．\n",
    "また，インライアの対応点１点あたりの距離の平均値を計算します．\n",
    "この値が小さいほど，良い変換行列ということになります．\n",
    "\n",
    "サンプリングと評価の試行を繰り返して，最も良い変換行列を最終結果とします．\n",
    "\n",
    "\n",
    "では，この処理を実行してみましょう．\n",
    "o3d.pipelines.registration.registration_ransac_based_on_correspondenceを使います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_threshold = voxel_size*1.5\n",
    "result = o3d.pipelines.registration.registration_ransac_based_on_correspondence(\n",
    "        s_kp, t_kp, corrs,\n",
    "        distance_threshold,\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPoint(False),\n",
    "        ransac_n = 3, \n",
    "        checkers = [\n",
    "            o3d.pipelines.registration.CorrespondenceCheckerBasedOnEdgeLength(0.9),\n",
    "            o3d.pipelines.registration.CorrespondenceCheckerBasedOnDistance(distance_threshold)\n",
    "        ], \n",
    "        criteria = o3d.pipelines.registration.RANSACConvergenceCriteria(100000, 0.999)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この関数は引数が多いので，順に説明します．\n",
    "- s_kp, t_kp, corrs\n",
    "\t- RANSACのために必要な材料です．それぞれ，source側のキーポイント，target側のキーポイント，対応点探索によって得られた対応点のインデクスのリストです．\n",
    "- distance_threshold\n",
    "\t- インライアと判定する距離（マージン）のしきい値です．\n",
    "- ransac_n\n",
    "\t- 姿勢変換行列の計算のためにサンプリングする対応点の個数です．\n",
    "- checkers\n",
    "\t- 枝刈り処理に使われる条件です．ここでいう枝刈り処理とは，「サンプリング」と「評価」の間に簡単な条件をチェックすることによって，調べる必要のない（外れ値を含んだ）サンプルを除外する処理のことです．この処理により，無駄な「評価」をスキップすることができ，RANSACが高速化されます．ここでは，「EdgeLength」「Distance」を使った枝刈りをおこないます.「Distance」では，変換行列によってサンプリングした対応点(ransac_n点)を変換し，距離が近いかどうかを判定します．近ければ，有望な変換行列とみなします．「EdgeLength」では，サンプリングした対応点の配置の関係性を評価します．EdgeLengthとは，片方の点群内で対応点間の距離のことです．source内で2点選び，その距離を$e_s$とし，targetでも同様に距離を計算し$e_t$とします．もし，「サンプリング」によって選ばれた対応点がsourceとtargetの同一地点を指していれば，$e_s$と$e_t$の値は類似するはずです．これを条件に枝刈りをおこないます．\n",
    "- criteria\n",
    "\t- RANSACの終了条件を指定しています．第１引数は試行回数の最大数です．試行中に得られた解の良さを利用して試行回数が変更されます．良い解を早い段階で発見した場合は，早期に試行が終了します．この時に使われる値が第２引数です．これら二つの値が大きいほど試行回数が増えることになるので，精度の良い解が得られる可能性が高まります．小さくすれば，その分，早く結果を得られます．\n",
    "\n",
    "返却値である変数resultには，RANSACの結果が保存されています．\n",
    "- correspondence_set: インライアと判定された対応点のインデクスのリスト\n",
    "- fitness: インライア数/対応点数の値．大きいほど良い．\n",
    "- inlier_rmse: インライアの平均二条誤差．小さいほど良い．\n",
    "- tranformation：$4 \\times 4の$変換行列\n",
    "\n",
    "では，結果を確認しましょう．\n",
    "まずはインライアの対応点を確認します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_set = create_lineset_from_correspondences( result.correspondence_set, s_kp, t_kp, initial_trans )\n",
    "draw_registration_result([source,s_kp], [target,t_kp, line_set], initial_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "２つの点群の同一地点を結んだ対応点だけが残されているように見えます．この対応点であれば，精度の良い変換行列が計算できそうです．では，結果を表示してみましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_registration_result([source], [target], result.transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "この図の通り，RANSACを使う前よりも精度の良い位置合わせができたことがわかります．\n",
    "さらに精度を高める場合には，この結果を初期値としてICPアルゴリズムを実行することが良くとられる方法です．\n",
    "\n",
    "ここまでの処理のサンプルコードの実行方法は次の通りです．引数としてマッチングさせる2つの点群データのパスを指定しています．\n",
    "```bash\n",
    "python o3d_keypoint_matching.py \\ \n",
    "../3rdparty/Open3D/examples/test_data/ICP/cloud_bin_0.pcd \\ \n",
    "../3rdparty/Open3D/examples/test_data/ICP/cloud_bin_1.pcd\n",
    "```\n",
    "\n",
    "補足ですが，open3dでは，対応点探索とRANSACによる姿勢計算をまとめて実行する方法も用意されています．\n",
    "以下の通りです．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_threshold = voxel_size*1.5\n",
    "result = o3d.pipelines.registration.registration_ransac_based_on_feature_matching(\n",
    "        s_kp, t_kp, s_feature, t_feature, True,\n",
    "        distance_threshold,\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPoint(False),\n",
    "        ransac_n = 3, \n",
    "        checkers = [\n",
    "            o3d.pipelines.registration.CorrespondenceCheckerBasedOnEdgeLength(0.9),\n",
    "            o3d.pipelines.registration.CorrespondenceCheckerBasedOnDistance(distance_threshold)\n",
    "        ], \n",
    "        criteria = o3d.pipelines.registration.RANSACConvergenceCriteria(100000, 0.999)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_registration_result([source], [target], result.transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
